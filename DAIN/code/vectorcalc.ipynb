{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24b44869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "# from ffpyplayer.player import MediaPlayer\n",
    "# from mediapipe.python.solutions import hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d71c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x1,y1,x2,y2): # 비교를 위한 거리 계산 함수\n",
    "    return math.sqrt( math.pow(x1-x2,2) + math.pow(y1-y2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b77c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareIndex = [[18,4],[6,8],[10,12],[14,16],[18,20]] # 마디 index 번호\n",
    "hand_open = [False,False,False,False,False] # False로 초기화\n",
    "gesture = [\n",
    "    [False,True,True,False,False,\"Yeah\"],\n",
    "    [True,False,False,False,False,\"Good\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d94ced64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mpHands = mp.solutions.hands\n",
    "my_hands = mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7813d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "#control frame rate\n",
    "cap = cv2.VideoCapture('../sample_dance/videoplayback.mp4') # jupyter \n",
    "# cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "#cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "frame_counter = 0\n",
    "frameTime = int((1/fps)*1000)  #time of each frame (ms단위, 몇ms당 1frame으로 할지 설정)\n",
    "print(fps)\n",
    "extract_time_by_per_frame = 1 #몇프레임 당 한번 측정할지 조절 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6e7ba30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ###start step2 --> 영상 추출\n",
    "# #extract_file = open('DAIN/extractsamples/result.txt','w')\n",
    "# cap = cv2.VideoCapture('../sample_dance/videoplayback.mp4') # jupyter \n",
    "# # cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "# #cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "\n",
    "result = np.array([]) # 추출된 영상의 전체 넘파이 배열\n",
    "bone_index = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28] # 필요한 관절 번호\n",
    "\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame_counter += 1 #increase frame counter\n",
    "\n",
    "        #Recolor image to RGB\n",
    "        if ret:\n",
    "            if frame_counter % extract_time_by_per_frame == 0:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False\n",
    "\n",
    "                #Detection\n",
    "                results = pose.process(image)\n",
    "\n",
    "                #Recolor image RGB to BGR\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                #Extract landmarks\n",
    "                try: #success to extract landmarks\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    ret = []\n",
    "                    \n",
    "                    # # 11 12 13 14 15 16 23 24 25 26 27 28\n",
    "                    for i in bone_index: # 필요한 부분의 관절의 정보만\n",
    "                        temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "                        ret = np.append(ret,temp,axis = 0)\n",
    "                        \n",
    "                        \n",
    "#                     for i in range(len(landmarks)):\n",
    "#                         temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "#                         ret = np.append(ret,temp,axis = 0)\n",
    "                    \n",
    "                    #print(ret)\n",
    "                    result = np.append(result,ret,axis=0)\n",
    "                    #print(\"extracting\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        else: #no next frame (end point of video)\n",
    "            break\n",
    "            #Show video on screen\n",
    "        cv2.imshow('dance_file',frame)\n",
    "        \n",
    "        if cv2.waitKey(frameTime) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #extract_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b229cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****extract success*****\n",
      "[[ 0.56878728  0.36568233 -0.15027806]\n",
      " [ 0.49939519  0.37074822 -0.1233276 ]\n",
      " [ 0.58659947  0.45521194 -0.125323  ]\n",
      " [ 0.49071422  0.46475789 -0.09052768]\n",
      " [ 0.57153028  0.53883195 -0.20202181]\n",
      " [ 0.50347084  0.54858035 -0.17785837]\n",
      " [ 0.55581081  0.54644543 -0.01378596]\n",
      " [ 0.51830608  0.54936028  0.01372805]\n",
      " [ 0.55500901  0.69884503 -0.0302585 ]\n",
      " [ 0.51074547  0.70174855  0.00140939]\n",
      " [ 0.55349058  0.81681919  0.13963871]\n",
      " [ 0.52232307  0.80851245  0.1724093 ]]\n",
      "(299, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "print('*****extract success*****')\n",
    "z = result.shape[0]/(len(bone_index)*3)\n",
    "result = np.reshape(result,(int(z),len(bone_index),3)) # frame 수,  tracking좌표 수, xyz \n",
    "print(result[0])\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7a4a2",
   "metadata": {},
   "source": [
    "# body index\n",
    "body_index = [\n",
    "    \n",
    "    [11,13,15], # left arm\n",
    "    [12,14,16], # right arm\n",
    "    [23,25,27], # left leg\n",
    "    [24,26,28], # right leg\n",
    "    \n",
    "    [14,12,24],\n",
    "    [13,11,23],\n",
    "    [23,24,26],\n",
    "    [24,23,25],\n",
    "    \n",
    "]\n",
    "l1 = result[0][11]\n",
    "l2 = result[0][13]\n",
    "l3 = result[0][15]\n",
    "\n",
    "\n",
    "ll1 = l1-l2\n",
    "ll2 = l3-l2\n",
    "\n",
    "innerAB = np.dot(ll1,ll2)\n",
    "AB = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "angle = np.arccos(innerAB/AB)\n",
    "print(angle) # radian\n",
    "print(angle/np.pi*180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9108c",
   "metadata": {},
   "source": [
    "# 두번째 영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af976b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.97002997002997\n"
     ]
    }
   ],
   "source": [
    "#control frame rate\n",
    "cap = cv2.VideoCapture('../../HSLEE/demo/dance/sample.mp4') # jupyter \n",
    "# cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "#cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "frame_counter = 0\n",
    "frameTime = int((1/fps)*1000)  #time of each frame (ms단위, 몇ms당 1frame으로 할지 설정)\n",
    "print(fps)\n",
    "extract_time_by_per_frame = 1 #몇프레임 당 한번 측정할지 조절 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "167c18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###start step2 --> 영상 추출\n",
    "# #extract_file = open('DAIN/extractsamples/result.txt','w')\n",
    "# cap = cv2.VideoCapture('../sample_dance/videoplayback.mp4') # jupyter \n",
    "# # cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "# #cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "\n",
    "user_video_result = np.array([]) # 추출된 영상의 전체 넘파이 배열\n",
    "\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame_counter += 1 #increase frame counter\n",
    "\n",
    "        #Recolor image to RGB\n",
    "        if ret:\n",
    "            if frame_counter % extract_time_by_per_frame == 0:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False\n",
    "\n",
    "                #Detection\n",
    "                results = pose.process(image)\n",
    "\n",
    "                #Recolor image RGB to BGR\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                #Extract landmarks\n",
    "                try: #success to extract landmarks\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    ret = []\n",
    "                    \n",
    "                    # # 11 12 13 14 15 16 23 24 25 26 27 28\n",
    "                    for i in bone_index: # 필요한 부분의 관절의 정보만\n",
    "                        temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "                        ret = np.append(ret,temp,axis = 0)\n",
    "                        \n",
    "                        \n",
    "#                     for i in range(len(landmarks)):\n",
    "#                         temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "#                         ret = np.append(ret,temp,axis = 0)\n",
    "                    \n",
    "                    #print(ret)\n",
    "                    user_video_result = np.append(user_video_result,ret,axis=0)\n",
    "                    #print(\"extracting\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        else: #no next frame (end point of video)\n",
    "            break\n",
    "            #Show video on screen\n",
    "        cv2.imshow('dance_file',frame)\n",
    "        \n",
    "        if cv2.waitKey(frameTime) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #extract_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9387d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****extract success*****\n",
      "[[ 0.5555228   0.29065996 -0.01759281]\n",
      " [ 0.46472317  0.29614663 -0.01432965]\n",
      " [ 0.56513101  0.42264941  0.0045408 ]\n",
      " [ 0.45936608  0.42759302  0.01899337]\n",
      " [ 0.57247478  0.53129256 -0.11659247]\n",
      " [ 0.45542723  0.53898442 -0.07957601]\n",
      " [ 0.53683549  0.53764474 -0.00246118]\n",
      " [ 0.48751217  0.53848773  0.00252586]\n",
      " [ 0.53348666  0.73138911 -0.04371687]\n",
      " [ 0.48928958  0.73172796 -0.02784941]\n",
      " [ 0.52787054  0.88830447  0.11362548]\n",
      " [ 0.49267215  0.88548726  0.12154137]]\n",
      "(747, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "print('*****extract success*****')\n",
    "z = user_video_result.shape[0]/(len(bone_index)*3)\n",
    "user_video_result = np.reshape(user_video_result,(int(z),len(bone_index),3)) # frame 수,  tracking좌표 수, xyz \n",
    "print(user_video_result[0])\n",
    "print(user_video_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0ab87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d30c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a27d6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_index = [\n",
    "    [0,2,4], # left arm\n",
    "    [1,3,4], # right arm\n",
    "    [6,8,10], # left leg\n",
    "    [7,9,11], # right leg\n",
    "\n",
    "    [3,1,7],\n",
    "    [2,0,6],\n",
    "    [6,7,9],\n",
    "    [7,6,8],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ff86586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_video_result = result # 녹화된 비디오\n",
    "total_point = 0 # 총 포인트\n",
    "\n",
    "# 프레임 수가 작은 것을 기준으로 \n",
    "length = len(result)\n",
    "if(len(result)>= len(user_video_result)):\n",
    "    length = len(user_video_result)\n",
    "\n",
    "\n",
    "for n in range(length-1, -1, -1):\n",
    "        \n",
    "    for index in body_index:\n",
    "        l1 = result[n][index[0]]\n",
    "        l2 = result[n][index[1]]\n",
    "        l3 = result[n][index[2]]\n",
    "\n",
    "        ll1 = l1-l2\n",
    "        ll2 = l3-l2\n",
    "        \n",
    "        innerREAL = np.dot(ll1,ll2)\n",
    "        REAL = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "        angleREAL = np.arccos(innerREAL/REAL)/np.pi*180\n",
    "        #print(angleREAL/np.pi*180)\n",
    "\n",
    "\n",
    "        u1 = user_video_result[n][index[0]]\n",
    "        u2 = user_video_result[n][index[1]]\n",
    "        u3 = user_video_result[n][index[2]]\n",
    "\n",
    "        uu1 = u1-u2\n",
    "        uu2 = u3-u2\n",
    "        \n",
    "        innerUSER = np.dot(uu1,uu2)\n",
    "        USER = np.linalg.norm(uu1) * np.linalg.norm(uu2)\n",
    "        angleUSER = np.arccos(innerUSER/USER)/np.pi*180\n",
    "        #print(angleUSER)\n",
    "\n",
    "\n",
    "        if angleUSER <= angleREAL + 10.0 and angleUSER >= angleREAL-10.0:\n",
    "            total_point+=1\n",
    "            \n",
    " # 8 : 비교 각도의 개수 len(result) : frame 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8b3dc697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556\n",
      "23.244147157190636\n"
     ]
    }
   ],
   "source": [
    "print(total_point)\n",
    "print((total_point / (8 * length ) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c052677",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for num, r in enumerate(result):\n",
    "    \n",
    "    for index in body_index:\n",
    "        l1 = r[index[0]]\n",
    "        l2 = r[index[1]]\n",
    "        l3 = r[index[2]]\n",
    "        \n",
    "        ll1 = l1-l2\n",
    "        ll2 = l3-l2\n",
    "        \n",
    "        innerAB = np.dot(ll1,ll2)\n",
    "        AB = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "        angle = np.arccos(innerAB/AB)\n",
    "        print(angle/np.pi*180)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9fa99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53d3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
