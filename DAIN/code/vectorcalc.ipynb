{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24b44869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "# from ffpyplayer.player import MediaPlayer\n",
    "# from mediapipe.python.solutions import hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d71c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x1,y1,x2,y2): # 비교를 위한 거리 계산 함수\n",
    "    return math.sqrt( math.pow(x1-x2,2) + math.pow(y1-y2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b77c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareIndex = [[18,4],[6,8],[10,12],[14,16],[18,20]] # 마디 index 번호\n",
    "hand_open = [False,False,False,False,False] # False로 초기화\n",
    "gesture = [\n",
    "    [False,True,True,False,False,\"Yeah\"],\n",
    "    [True,False,False,False,False,\"Good\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d94ced64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mpHands = mp.solutions.hands\n",
    "my_hands = mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51daa35",
   "metadata": {},
   "source": [
    "# 첫번째 영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c7813d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "#control frame rate\n",
    "cap = cv2.VideoCapture('../sample_dance/dyna1_30.mp4') # jupyter \n",
    "# cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "#cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "frame_counter = 0\n",
    "frameTime = int((1/fps)*1000)  #time of each frame (ms단위, 몇ms당 1frame으로 할지 설정)\n",
    "print(fps)\n",
    "extract_time_by_per_frame = 1 #몇프레임 당 한번 측정할지 조절 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f6e7ba30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ###start step2 --> 영상 추출\n",
    "# #extract_file = open('DAIN/extractsamples/result.txt','w')\n",
    "# cap = cv2.VideoCapture('../sample_dance/videoplayback.mp4') # jupyter \n",
    "# # cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "# #cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "\n",
    "result = np.array([]) # 추출된 영상의 전체 넘파이 배열\n",
    "bone_index = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28] # 필요한 관절 번호\n",
    "\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame_counter += 1 #increase frame counter\n",
    "\n",
    "        #Recolor image to RGB\n",
    "        if ret:\n",
    "            if frame_counter % extract_time_by_per_frame == 0:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False\n",
    "\n",
    "                #Detection\n",
    "                results = pose.process(image)\n",
    "\n",
    "                #Recolor image RGB to BGR\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                #Extract landmarks\n",
    "                try: #success to extract landmarks\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    ret = []\n",
    "                    \n",
    "                    # # 11 12 13 14 15 16 23 24 25 26 27 28\n",
    "                    for i in bone_index: # 필요한 부분의 관절의 정보만\n",
    "                        temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "                        ret = np.append(ret,temp,axis = 0)\n",
    "                        \n",
    "                        \n",
    "#                     for i in range(len(landmarks)):\n",
    "#                         temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "#                         ret = np.append(ret,temp,axis = 0)\n",
    "                    \n",
    "                    #print(ret)\n",
    "                    result = np.append(result,ret,axis=0)\n",
    "                    #print(\"extracting\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        else: #no next frame (end point of video)\n",
    "            break\n",
    "            #Show video on screen\n",
    "        cv2.imshow('dance_file',frame)\n",
    "        \n",
    "        if cv2.waitKey(frameTime) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #extract_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d40bd622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****extract success*****\n",
      "[[ 0.46507198  0.3546381   0.06392627]\n",
      " [ 0.53517652  0.35496491 -0.06406817]\n",
      " [ 0.46453208  0.44917837  0.11296497]\n",
      " [ 0.53740734  0.46542028 -0.06063438]\n",
      " [ 0.47957745  0.4979032   0.17253238]\n",
      " [ 0.54777753  0.54769289 -0.01412602]\n",
      " [ 0.48015919  0.55657172  0.03743545]\n",
      " [ 0.51852137  0.56020677 -0.0373728 ]\n",
      " [ 0.47822624  0.70837885  0.04842366]\n",
      " [ 0.50764787  0.71957946 -0.01146149]\n",
      " [ 0.48129281  0.85916233  0.09490544]\n",
      " [ 0.49353781  0.87991816  0.02743725]]\n",
      "(312, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "print('*****extract success*****')\n",
    "z = result.shape[0]/(len(bone_index)*3)\n",
    "result = np.reshape(result,(int(z),len(bone_index),3)) # frame 수,  tracking좌표 수, xyz \n",
    "print(result[0])\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "03c37db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be202501",
   "metadata": {},
   "source": [
    "# body index\n",
    "body_index = [\n",
    "    \n",
    "    [11,13,15], # left arm\n",
    "    [12,14,16], # right arm\n",
    "    [23,25,27], # left leg\n",
    "    [24,26,28], # right leg\n",
    "    \n",
    "    [14,12,24],\n",
    "    [13,11,23],\n",
    "    [23,24,26],\n",
    "    [24,23,25],\n",
    "    \n",
    "]\n",
    "l1 = result[0][11]\n",
    "l2 = result[0][13]\n",
    "l3 = result[0][15]\n",
    "\n",
    "\n",
    "ll1 = l1-l2\n",
    "ll2 = l3-l2\n",
    "\n",
    "innerAB = np.dot(ll1,ll2)\n",
    "AB = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "angle = np.arccos(innerAB/AB)\n",
    "print(angle) # radian\n",
    "print(angle/np.pi*180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58baf3f",
   "metadata": {},
   "source": [
    "# 두번째 영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2d775417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "#control frame rate\n",
    "cap = cv2.VideoCapture('../sample_dance/dyna2.mp4') #('../../HSLEE/demo/dance/sample.mp4') # jupyter \n",
    "# cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "#cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "frame_counter = 0\n",
    "frameTime = int((1/fps)*1000)  #time of each frame (ms단위, 몇ms당 1frame으로 할지 설정)\n",
    "print(fps)\n",
    "extract_time_by_per_frame = 1 #몇프레임 당 한번 측정할지 조절 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "56907f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###start step2 --> 영상 추출\n",
    "# #extract_file = open('DAIN/extractsamples/result.txt','w')\n",
    "# cap = cv2.VideoCapture('../sample_dance/videoplayback.mp4') # jupyter \n",
    "# # cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "# #cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "\n",
    "user_video_result = np.array([]) # 추출된 영상의 전체 넘파이 배열\n",
    "\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame_counter += 1 #increase frame counter\n",
    "\n",
    "        #Recolor image to RGB\n",
    "        if ret:\n",
    "            if frame_counter % extract_time_by_per_frame == 0:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False\n",
    "\n",
    "                #Detection\n",
    "                results = pose.process(image)\n",
    "\n",
    "                #Recolor image RGB to BGR\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                #Extract landmarks\n",
    "                try: #success to extract landmarks\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    ret = []\n",
    "                    \n",
    "                    # # 11 12 13 14 15 16 23 24 25 26 27 28\n",
    "                    for i in bone_index: # 필요한 부분의 관절의 정보만\n",
    "                        temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "                        ret = np.append(ret,temp,axis = 0)\n",
    "                        \n",
    "                        \n",
    "#                     for i in range(len(landmarks)):\n",
    "#                         temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "#                         ret = np.append(ret,temp,axis = 0)\n",
    "                    \n",
    "                    #print(ret)\n",
    "                    user_video_result = np.append(user_video_result,ret,axis=0)\n",
    "                    #print(\"extracting\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        else: #no next frame (end point of video)\n",
    "            break\n",
    "            #Show video on screen\n",
    "        cv2.imshow('dance_file',frame)\n",
    "        \n",
    "        if cv2.waitKey(frameTime) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #extract_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3b2a0e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****extract success*****\n",
      "[[ 0.45640907  0.52090657 -0.021587  ]\n",
      " [ 0.50814033  0.51846415 -0.03642946]\n",
      " [ 0.46140724  0.61364704 -0.02786098]\n",
      " [ 0.50287867  0.61906171 -0.0586761 ]\n",
      " [ 0.47413212  0.66606671 -0.02912012]\n",
      " [ 0.51249516  0.67429352 -0.05859148]\n",
      " [ 0.47191724  0.69402885 -0.00483433]\n",
      " [ 0.50038564  0.69343954  0.00485449]\n",
      " [ 0.47499809  0.82567322  0.02512417]\n",
      " [ 0.49559537  0.82578433  0.02651009]\n",
      " [ 0.4776004   0.95226687  0.05504317]\n",
      " [ 0.4945775   0.95147789  0.04697707]]\n",
      "(314, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "print('*****extract success*****')\n",
    "z = user_video_result.shape[0]/(len(bone_index)*3)\n",
    "user_video_result = np.reshape(user_video_result,(int(z),len(bone_index),3)) # frame 수,  tracking좌표 수, xyz \n",
    "print(user_video_result[0])\n",
    "print(user_video_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "936b4f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_video_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb13b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f1c77f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_index = [\n",
    "    [0,2,4], # left arm\n",
    "    [1,3,4], # right arm\n",
    "    [6,8,10], # left leg\n",
    "    [7,9,11], # right leg\n",
    "\n",
    "    [3,1,7],\n",
    "    [2,0,6],\n",
    "    [6,7,9],\n",
    "    [7,6,8],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8f7b339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "# user_video_result = result # 녹화된 비디오\n",
    "total_point = 0 # 총 포인트\n",
    "\n",
    "# 프레임 수가 작은 것을 기준으로 \n",
    "length = len(result)\n",
    "print(length)\n",
    "\n",
    "if( len(result) >= len(user_video_result)):\n",
    "    length = len(user_video_result)\n",
    "    print(len(user_video_result))\n",
    "\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3f98075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for n in range(length-1, -1, -1):\n",
    "        \n",
    "    for index in body_index:\n",
    "        l1 = result[n][index[0]]\n",
    "        l2 = result[n][index[1]]\n",
    "        l3 = result[n][index[2]]\n",
    "\n",
    "        ll1 = l1-l2\n",
    "        ll2 = l3-l2\n",
    "        \n",
    "        innerREAL = np.dot(ll1,ll2)\n",
    "        REAL = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "        angleREAL = np.arccos(innerREAL/REAL)/np.pi*180\n",
    "        #print(angleREAL/np.pi*180)\n",
    "\n",
    "\n",
    "        u1 = user_video_result[n][index[0]]\n",
    "        u2 = user_video_result[n][index[1]]\n",
    "        u3 = user_video_result[n][index[2]]\n",
    "\n",
    "        uu1 = u1-u2\n",
    "        uu2 = u3-u2\n",
    "        \n",
    "        innerUSER = np.dot(uu1,uu2)\n",
    "        USER = np.linalg.norm(uu1) * np.linalg.norm(uu2)\n",
    "        angleUSER = np.arccos(innerUSER/USER)/np.pi*180\n",
    "        #print(angleUSER)\n",
    "\n",
    "\n",
    "        if angleUSER <= angleREAL + 30.0 and angleUSER >= angleREAL - 30.0:\n",
    "            total_point+=1\n",
    "            \n",
    " # 8 : 비교 각도의 개수 len(result) : frame 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4a1da5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8280\n",
      "331.7307692307692\n"
     ]
    }
   ],
   "source": [
    "print(total_point)\n",
    "print((total_point / (8 * length ) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d54b9",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for num, r in enumerate(result):\n",
    "    \n",
    "    for index in body_index:\n",
    "        l1 = r[index[0]]\n",
    "        l2 = r[index[1]]\n",
    "        l3 = r[index[2]]\n",
    "        \n",
    "        ll1 = l1-l2\n",
    "        ll2 = l3-l2\n",
    "        \n",
    "        innerAB = np.dot(ll1,ll2)\n",
    "        AB = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "        angle = np.arccos(innerAB/AB)\n",
    "        print(angle/np.pi*180)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292bc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53d3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
