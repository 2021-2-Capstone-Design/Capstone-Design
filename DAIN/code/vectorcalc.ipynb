{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24b44869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "# from ffpyplayer.player import MediaPlayer\n",
    "# from mediapipe.python.solutions import hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d71c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x1,y1,x2,y2): # 비교를 위한 거리 계산 함수\n",
    "    return math.sqrt( math.pow(x1-x2,2) + math.pow(y1-y2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b77c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareIndex = [[18,4],[6,8],[10,12],[14,16],[18,20]] # 마디 index 번호\n",
    "hand_open = [False,False,False,False,False] # False로 초기화\n",
    "gesture = [\n",
    "    [False,True,True,False,False,\"Yeah\"],\n",
    "    [True,False,False,False,False,\"Good\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d94ced64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mpHands = mp.solutions.hands\n",
    "my_hands = mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addec82",
   "metadata": {},
   "source": [
    "# 첫번째 영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c7813d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.763310344827588\n"
     ]
    }
   ],
   "source": [
    "#control frame rate\n",
    "cap = cv2.VideoCapture('../sample_dance/test.mp4') # jupyter \n",
    "# cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "#cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "frame_counter = 0\n",
    "frameTime = int((1/fps)*1000)  #time of each frame (ms단위, 몇ms당 1frame으로 할지 설정)\n",
    "print(fps)\n",
    "extract_time_by_per_frame = 1 #몇프레임 당 한번 측정할지 조절 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f6e7ba30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ###start step2 --> 영상 추출\n",
    "# #extract_file = open('DAIN/extractsamples/result.txt','w')\n",
    "# cap = cv2.VideoCapture('../sample_dance/videoplayback.mp4') # jupyter \n",
    "# # cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "# #cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "\n",
    "result = np.array([]) # 추출된 영상의 전체 넘파이 배열\n",
    "bone_index = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28] # 필요한 관절 번호\n",
    "\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame_counter += 1 #increase frame counter\n",
    "\n",
    "        #Recolor image to RGB\n",
    "        if ret:\n",
    "            if frame_counter % extract_time_by_per_frame == 0:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False\n",
    "\n",
    "                #Detection\n",
    "                results = pose.process(image)\n",
    "\n",
    "                #Recolor image RGB to BGR\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                #Extract landmarks\n",
    "                try: #success to extract landmarks\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    ret = []\n",
    "                    \n",
    "                    # # 11 12 13 14 15 16 23 24 25 26 27 28\n",
    "                    for i in bone_index: # 필요한 부분의 관절의 정보만\n",
    "                        temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "                        ret = np.append(ret,temp,axis = 0)\n",
    "                        \n",
    "                        \n",
    "#                     for i in range(len(landmarks)):\n",
    "#                         temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "#                         ret = np.append(ret,temp,axis = 0)\n",
    "                    \n",
    "                    #print(ret)\n",
    "                    result = np.append(result,ret,axis=0)\n",
    "                    #print(\"extracting\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        else: #no next frame (end point of video)\n",
    "            break\n",
    "            #Show video on screen\n",
    "        cv2.imshow('dance_file',frame)\n",
    "        \n",
    "        if cv2.waitKey(frameTime) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #extract_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aeee4779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****extract success*****\n",
      "[[ 0.51516652  0.28044745 -0.02487076]\n",
      " [ 0.42421359  0.29118058 -0.04777415]\n",
      " [ 0.54657716  0.38309592  0.00263975]\n",
      " [ 0.40421957  0.40315089 -0.06857859]\n",
      " [ 0.5491789   0.49058554 -0.11524694]\n",
      " [ 0.41963249  0.50564843 -0.21349053]\n",
      " [ 0.50825787  0.53166103  0.01178689]\n",
      " [ 0.45652923  0.54102325 -0.01172364]\n",
      " [ 0.52605563  0.70492083  0.02193459]\n",
      " [ 0.45664182  0.72638744 -0.01040249]\n",
      " [ 0.52904516  0.85086763  0.17341679]\n",
      " [ 0.46009231  0.87993968  0.12814207]]\n",
      "(144, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "print('*****extract success*****')\n",
    "z = result.shape[0]/(len(bone_index)*3)\n",
    "result = np.reshape(result,(int(z),len(bone_index),3)) # frame 수,  tracking좌표 수, xyz \n",
    "print(result[0])\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1e5eae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706bcf5",
   "metadata": {},
   "source": [
    "# body index\n",
    "body_index = [\n",
    "    \n",
    "    [11,13,15], # left arm\n",
    "    [12,14,16], # right arm\n",
    "    [23,25,27], # left leg\n",
    "    [24,26,28], # right leg\n",
    "    \n",
    "    [14,12,24],\n",
    "    [13,11,23],\n",
    "    [23,24,26],\n",
    "    [24,23,25],\n",
    "    \n",
    "]\n",
    "l1 = result[0][11]\n",
    "l2 = result[0][13]\n",
    "l3 = result[0][15]\n",
    "\n",
    "\n",
    "ll1 = l1-l2\n",
    "ll2 = l3-l2\n",
    "\n",
    "innerAB = np.dot(ll1,ll2)\n",
    "AB = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "angle = np.arccos(innerAB/AB)\n",
    "print(angle) # radian\n",
    "print(angle/np.pi*180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711633ad",
   "metadata": {},
   "source": [
    "# 두번째 영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "32855b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.763\n"
     ]
    }
   ],
   "source": [
    "#control frame rate\n",
    "cap = cv2.VideoCapture('../extractsamples/right_hand.mp4') #('../../HSLEE/demo/dance/sample.mp4') # jupyter \n",
    "# cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "#cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "frame_counter = 0\n",
    "frameTime = int((1/fps)*1000)  #time of each frame (ms단위, 몇ms당 1frame으로 할지 설정)\n",
    "print(fps)\n",
    "extract_time_by_per_frame = 1 #몇프레임 당 한번 측정할지 조절 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d34e30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###start step2 --> 영상 추출\n",
    "# #extract_file = open('DAIN/extractsamples/result.txt','w')\n",
    "# cap = cv2.VideoCapture('../sample_dance/videoplayback.mp4') # jupyter \n",
    "# # cap = cv2.VideoCapture('./sample_dance/videoplayback.mp4') # vs\n",
    "\n",
    "# #cap = cv2.VideoCapture('C:/Users/yoond/바탕 화면/00CAPSTONE/Capstone-Design/DAIN/sample_dance/videoplayback.mp4')\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "\n",
    "user_video_result = np.array([]) # 추출된 영상의 전체 넘파이 배열\n",
    "\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame_counter += 1 #increase frame counter\n",
    "\n",
    "        #Recolor image to RGB\n",
    "        if ret:\n",
    "            if frame_counter % extract_time_by_per_frame == 0:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False\n",
    "\n",
    "                #Detection\n",
    "                results = pose.process(image)\n",
    "\n",
    "                #Recolor image RGB to BGR\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                #Extract landmarks\n",
    "                try: #success to extract landmarks\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    ret = []\n",
    "                    \n",
    "                    # # 11 12 13 14 15 16 23 24 25 26 27 28\n",
    "                    for i in bone_index: # 필요한 부분의 관절의 정보만\n",
    "                        temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "                        ret = np.append(ret,temp,axis = 0)\n",
    "                        \n",
    "                        \n",
    "#                     for i in range(len(landmarks)):\n",
    "#                         temp = np.array([landmarks[i].x,landmarks[i].y,landmarks[i].z], float)\n",
    "#                         ret = np.append(ret,temp,axis = 0)\n",
    "                    \n",
    "                    #print(ret)\n",
    "                    user_video_result = np.append(user_video_result,ret,axis=0)\n",
    "                    #print(\"extracting\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        else: #no next frame (end point of video)\n",
    "            break\n",
    "            #Show video on screen\n",
    "        cv2.imshow('dance_file',frame)\n",
    "        \n",
    "        if cv2.waitKey(frameTime) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #extract_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1aae6cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****extract success*****\n",
      "[[ 0.54547864  0.63593251  0.38282126]\n",
      " [ 0.59472173  0.63971823  0.39493582]\n",
      " [ 0.52933753  0.72320312  0.30912185]\n",
      " [ 0.59287816  0.70600742  0.31163007]\n",
      " [ 0.53707701  0.79179287  0.29898843]\n",
      " [ 0.59089911  0.82353771  0.30223212]\n",
      " [ 0.54853195  0.55195528  0.00391595]\n",
      " [ 0.59018207  0.5694052  -0.00371853]\n",
      " [ 0.50811416  0.53133494 -0.25418702]\n",
      " [ 0.59647721  0.57505041 -0.18689804]\n",
      " [ 0.53542233  0.4364664  -0.47262853]\n",
      " [ 0.57706994  0.61793572 -0.39385515]]\n",
      "(124, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "print('*****extract success*****')\n",
    "z = user_video_result.shape[0]/(len(bone_index)*3)\n",
    "user_video_result = np.reshape(user_video_result,(int(z),len(bone_index),3)) # frame 수,  tracking좌표 수, xyz \n",
    "print(user_video_result[0])\n",
    "print(user_video_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c87b32b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_video_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df555b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f15cdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_index = [\n",
    "    [0,2,4], # left arm\n",
    "    [1,3,4], # right arm\n",
    "    [6,8,10], # left leg\n",
    "    [7,9,11], # right leg\n",
    "\n",
    "    [3,1,7],\n",
    "    [2,0,6],\n",
    "    [6,7,9],\n",
    "    [7,6,8],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d69229b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "124\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "# user_video_result = result # 녹화된 비디오\n",
    "total_point = 0 # 총 포인트\n",
    "\n",
    "# 프레임 수가 작은 것을 기준으로 \n",
    "length = len(result)\n",
    "print(length)\n",
    "\n",
    "if( len(result) >= len(user_video_result)):\n",
    "    length = len(user_video_result)\n",
    "    print(len(user_video_result))\n",
    "\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8cecfd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for n in range(length-1, -1, -1):\n",
    "        \n",
    "    for index in body_index:\n",
    "        l1 = result[n][index[0]]\n",
    "        l2 = result[n][index[1]]\n",
    "        l3 = result[n][index[2]]\n",
    "\n",
    "        ll1 = l1-l2\n",
    "        ll2 = l3-l2\n",
    "        \n",
    "        innerREAL = np.dot(ll1,ll2)\n",
    "        REAL = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "        angleREAL = np.arccos(innerREAL/REAL)/np.pi*180\n",
    "        #print(angleREAL/np.pi*180)\n",
    "\n",
    "\n",
    "        u1 = user_video_result[n][index[0]]\n",
    "        u2 = user_video_result[n][index[1]]\n",
    "        u3 = user_video_result[n][index[2]]\n",
    "\n",
    "        uu1 = u1-u2\n",
    "        uu2 = u3-u2\n",
    "        \n",
    "        innerUSER = np.dot(uu1,uu2)\n",
    "        USER = np.linalg.norm(uu1) * np.linalg.norm(uu2)\n",
    "        angleUSER = np.arccos(innerUSER/USER)/np.pi*180\n",
    "        #print(angleUSER)\n",
    "\n",
    "\n",
    "        if angleUSER <= angleREAL + 30.0 and angleUSER >= angleREAL - 30.0:\n",
    "            total_point+=1\n",
    "            \n",
    " # 8 : 비교 각도의 개수 len(result) : frame 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dcf43f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n",
      "80.14112903225806\n"
     ]
    }
   ],
   "source": [
    "print(total_point)\n",
    "print((total_point / (8 * length ) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc4bf9",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for num, r in enumerate(result):\n",
    "    \n",
    "    for index in body_index:\n",
    "        l1 = r[index[0]]\n",
    "        l2 = r[index[1]]\n",
    "        l3 = r[index[2]]\n",
    "        \n",
    "        ll1 = l1-l2\n",
    "        ll2 = l3-l2\n",
    "        \n",
    "        innerAB = np.dot(ll1,ll2)\n",
    "        AB = np.linalg.norm(ll1) * np.linalg.norm(ll2)\n",
    "        angle = np.arccos(innerAB/AB)\n",
    "        print(angle/np.pi*180)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fbd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53d3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
